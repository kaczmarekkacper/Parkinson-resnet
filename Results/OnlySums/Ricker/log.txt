Epoch 1/250
train Loss: 0.6847 Acc: 56.8378%
val Loss: 0.7365 Acc: 50.2063%
Epoch 2/250
train Loss: 0.6659 Acc: 59.2085%
val Loss: 0.7527 Acc: 48.5557%
Epoch 3/250
train Loss: 0.6583 Acc: 59.6003%
val Loss: 0.8006 Acc: 46.2173%
Epoch 4/250
train Loss: 0.6483 Acc: 59.7962%
val Loss: 0.8071 Acc: 39.8900%
Epoch 5/250
train Loss: 0.6422 Acc: 61.5204%
val Loss: 0.8443 Acc: 42.3659%
Epoch 6/250
train Loss: 0.6439 Acc: 60.2861%
val Loss: 0.8137 Acc: 39.0646%
Epoch 7/250
train Loss: 0.6360 Acc: 62.5392%
val Loss: 0.7977 Acc: 43.1912%
Epoch 8/250
train Loss: 0.6325 Acc: 62.2453%
val Loss: 0.8070 Acc: 45.5296%
Epoch 9/250
train Loss: 0.6338 Acc: 61.5400%
val Loss: 0.7954 Acc: 48.0055%
Epoch 10/250
train Loss: 0.6312 Acc: 62.4216%
val Loss: 0.7933 Acc: 42.6410%
Epoch 11/250
train Loss: 0.6302 Acc: 62.0102%
val Loss: 0.7986 Acc: 48.2806%
Epoch 12/250
train Loss: 0.6286 Acc: 62.2845%
val Loss: 0.8050 Acc: 51.4443%
Epoch 13/250
train Loss: 0.6246 Acc: 62.6959%
val Loss: 0.8200 Acc: 43.3287%
Epoch 14/250
train Loss: 0.6207 Acc: 62.9310%
val Loss: 0.8262 Acc: 48.6933%
Epoch 15/250
train Loss: 0.6204 Acc: 63.5776%
val Loss: 0.8327 Acc: 52.4072%
Epoch 16/250
train Loss: 0.6196 Acc: 63.1270%
val Loss: 0.8223 Acc: 41.8157%
Epoch 17/250
train Loss: 0.6179 Acc: 62.7939%
val Loss: 0.8107 Acc: 44.9794%
Epoch 18/250
train Loss: 0.6210 Acc: 62.8135%
val Loss: 0.7895 Acc: 49.2435%
Epoch 19/250
train Loss: 0.6156 Acc: 63.8127%
val Loss: 0.8203 Acc: 53.0949%
Epoch 20/250
train Loss: 0.6174 Acc: 64.4397%
val Loss: 0.7984 Acc: 39.6149%
Epoch 21/250
train Loss: 0.6184 Acc: 63.7931%
val Loss: 0.7825 Acc: 49.1059%
Epoch 22/250
train Loss: 0.6172 Acc: 63.4600%
val Loss: 0.7971 Acc: 46.3549%
Epoch 23/250
train Loss: 0.6115 Acc: 64.2437%
val Loss: 0.7856 Acc: 53.7827%
Epoch 24/250
train Loss: 0.6148 Acc: 64.3417%
val Loss: 0.7929 Acc: 54.6080%
Epoch 25/250
train Loss: 0.6111 Acc: 63.3229%
val Loss: 0.7525 Acc: 52.5447%
Epoch 26/250
train Loss: 0.6139 Acc: 63.6560%
val Loss: 0.7560 Acc: 51.5818%
Epoch 27/250
train Loss: 0.6100 Acc: 64.5572%
val Loss: 0.7866 Acc: 48.5557%
Epoch 28/250
train Loss: 0.6083 Acc: 64.0086%
val Loss: 0.7873 Acc: 47.0426%
Epoch 29/250
train Loss: 0.6031 Acc: 64.6160%
val Loss: 0.8172 Acc: 54.1953%
Epoch 30/250
train Loss: 0.6044 Acc: 64.9687%
val Loss: 0.7757 Acc: 50.7565%
Epoch 31/250
train Loss: 0.6036 Acc: 65.3409%
val Loss: 0.7840 Acc: 50.2063%
Epoch 32/250
train Loss: 0.6021 Acc: 65.7328%
val Loss: 0.7957 Acc: 45.5296%
Epoch 33/250
train Loss: 0.6049 Acc: 64.8511%
val Loss: 0.7877 Acc: 48.0055%
Epoch 34/250
train Loss: 0.5997 Acc: 66.3205%
val Loss: 0.7614 Acc: 56.9464%
Epoch 35/250
train Loss: 0.6098 Acc: 65.2625%
val Loss: 0.7878 Acc: 48.4182%
Epoch 36/250
train Loss: 0.5987 Acc: 66.0658%
val Loss: 0.7920 Acc: 52.8198%
Epoch 37/250
train Loss: 0.6016 Acc: 64.8707%
val Loss: 0.7808 Acc: 57.9092%
Epoch 38/250
train Loss: 0.6005 Acc: 65.3801%
val Loss: 0.7777 Acc: 49.9312%
Epoch 39/250
train Loss: 0.5998 Acc: 64.9491%
val Loss: 0.8017 Acc: 51.9945%
Epoch 40/250
train Loss: 0.5970 Acc: 65.8111%
val Loss: 0.7963 Acc: 53.2325%
Epoch 41/250
train Loss: 0.5960 Acc: 66.3009%
val Loss: 0.7762 Acc: 49.9312%
Epoch 42/250
train Loss: 0.5969 Acc: 65.7132%
val Loss: 0.7711 Acc: 52.6823%
Epoch 43/250
train Loss: 0.5978 Acc: 65.8895%
val Loss: 0.7509 Acc: 53.6451%
Epoch 44/250
train Loss: 0.5876 Acc: 67.0455%
val Loss: 0.8074 Acc: 54.8831%
Epoch 45/250
train Loss: 0.6012 Acc: 64.8903%
val Loss: 0.7706 Acc: 53.0949%
Epoch 46/250
train Loss: 0.5959 Acc: 66.2422%
val Loss: 0.7952 Acc: 52.8198%
Epoch 47/250
train Loss: 0.5898 Acc: 67.0846%
val Loss: 0.7966 Acc: 54.3329%
Epoch 48/250
train Loss: 0.5907 Acc: 66.4773%
val Loss: 0.7591 Acc: 52.5447%
Epoch 49/250
train Loss: 0.5908 Acc: 66.2226%
val Loss: 0.7874 Acc: 54.7455%
Epoch 50/250
train Loss: 0.5938 Acc: 66.4381%
val Loss: 0.7778 Acc: 55.5708%
Epoch 51/250
train Loss: 0.5915 Acc: 65.9875%
val Loss: 0.7787 Acc: 53.5076%
Epoch 52/250
train Loss: 0.5892 Acc: 67.0259%
val Loss: 0.7457 Acc: 52.9574%
Epoch 53/250
train Loss: 0.5909 Acc: 67.0846%
val Loss: 0.7858 Acc: 52.8198%
Epoch 54/250
train Loss: 0.5866 Acc: 66.2813%
val Loss: 0.8045 Acc: 52.8198%
Epoch 55/250
train Loss: 0.5878 Acc: 66.7124%
val Loss: 0.7777 Acc: 53.2325%
Epoch 56/250
train Loss: 0.5818 Acc: 68.0251%
val Loss: 0.7582 Acc: 53.2325%
Epoch 57/250
train Loss: 0.5892 Acc: 66.6144%
val Loss: 0.7536 Acc: 54.8831%
Epoch 58/250
train Loss: 0.5837 Acc: 67.5157%
val Loss: 0.8029 Acc: 52.2696%
Epoch 59/250
train Loss: 0.5844 Acc: 66.0266%
val Loss: 0.7425 Acc: 56.2586%
Epoch 60/250
train Loss: 0.5835 Acc: 66.4773%
val Loss: 0.7862 Acc: 53.6451%
Epoch 61/250
train Loss: 0.5807 Acc: 67.3589%
val Loss: 0.7289 Acc: 57.3590%
Epoch 62/250
train Loss: 0.5888 Acc: 66.1246%
val Loss: 0.7432 Acc: 56.5337%
Epoch 63/250
train Loss: 0.5888 Acc: 66.3401%
val Loss: 0.7541 Acc: 55.8459%
Epoch 64/250
train Loss: 0.5812 Acc: 67.0455%
val Loss: 0.7609 Acc: 54.7455%
Epoch 65/250
train Loss: 0.5747 Acc: 67.0455%
val Loss: 0.8123 Acc: 54.6080%
Epoch 66/250
train Loss: 0.5796 Acc: 67.2022%
val Loss: 0.7851 Acc: 54.4704%
Epoch 67/250
train Loss: 0.5736 Acc: 67.6724%
val Loss: 0.7759 Acc: 53.6451%
Epoch 68/250
train Loss: 0.5778 Acc: 67.6332%
val Loss: 0.7681 Acc: 54.7455%
Epoch 69/250
train Loss: 0.5759 Acc: 66.9867%
val Loss: 0.7587 Acc: 54.8831%
Epoch 70/250
train Loss: 0.5779 Acc: 67.1826%
val Loss: 0.7530 Acc: 55.4333%
Epoch 71/250
train Loss: 0.5784 Acc: 67.3589%
val Loss: 0.7632 Acc: 57.0839%
Epoch 72/250
train Loss: 0.5754 Acc: 67.9271%
val Loss: 0.7702 Acc: 55.4333%
Epoch 73/250
train Loss: 0.5837 Acc: 66.9671%
val Loss: 0.7507 Acc: 54.8831%
Epoch 74/250
train Loss: 0.5717 Acc: 67.8096%
val Loss: 0.7309 Acc: 55.1582%
Epoch 75/250
train Loss: 0.5765 Acc: 67.0455%
val Loss: 0.7669 Acc: 55.1582%
Epoch 76/250
train Loss: 0.5750 Acc: 68.3386%
val Loss: 0.8003 Acc: 54.4704%
Epoch 77/250
train Loss: 0.5754 Acc: 67.7116%
val Loss: 0.7382 Acc: 55.5708%
Epoch 78/250
train Loss: 0.5771 Acc: 67.7312%
val Loss: 0.7616 Acc: 55.9835%
Epoch 79/250
train Loss: 0.5713 Acc: 67.2414%
val Loss: 0.7347 Acc: 57.4966%
Epoch 80/250
train Loss: 0.5742 Acc: 67.0259%
val Loss: 0.7452 Acc: 59.6974%
Epoch 81/250
train Loss: 0.5669 Acc: 68.0447%
val Loss: 0.7226 Acc: 57.2215%
Epoch 82/250
train Loss: 0.5692 Acc: 67.8096%
val Loss: 0.7111 Acc: 58.3219%
Epoch 83/250
train Loss: 0.5702 Acc: 67.7704%
val Loss: 0.7451 Acc: 59.4223%
Epoch 84/250
train Loss: 0.5720 Acc: 67.7312%
val Loss: 0.7677 Acc: 57.4966%
Epoch 85/250
train Loss: 0.5683 Acc: 68.1426%
val Loss: 0.7182 Acc: 59.2847%
Epoch 86/250
train Loss: 0.5692 Acc: 67.7900%
val Loss: 0.7373 Acc: 55.8459%
Epoch 87/250
train Loss: 0.5713 Acc: 67.7900%
val Loss: 0.7550 Acc: 58.5970%
Epoch 88/250
train Loss: 0.5669 Acc: 68.7696%
val Loss: 0.7644 Acc: 57.4966%
Epoch 89/250
train Loss: 0.5704 Acc: 68.5149%
val Loss: 0.7656 Acc: 58.5970%
Epoch 90/250
train Loss: 0.5711 Acc: 67.4961%
val Loss: 0.7755 Acc: 56.1210%
Epoch 91/250
train Loss: 0.5738 Acc: 67.9859%
val Loss: 0.7428 Acc: 57.2215%
Epoch 92/250
train Loss: 0.5705 Acc: 68.3386%
val Loss: 0.7291 Acc: 57.6341%
Epoch 93/250
train Loss: 0.5658 Acc: 68.6520%
val Loss: 0.7343 Acc: 60.2476%
Epoch 94/250
train Loss: 0.5567 Acc: 68.7892%
val Loss: 0.7376 Acc: 57.9092%
Epoch 95/250
train Loss: 0.5642 Acc: 69.2398%
val Loss: 0.7233 Acc: 60.5227%
Epoch 96/250
train Loss: 0.5648 Acc: 67.8683%
val Loss: 0.6967 Acc: 61.6231%
Epoch 97/250
train Loss: 0.5589 Acc: 68.9851%
val Loss: 0.7633 Acc: 58.1843%
Epoch 98/250
train Loss: 0.5629 Acc: 68.7696%
val Loss: 0.7492 Acc: 57.3590%
Epoch 99/250
train Loss: 0.5606 Acc: 68.4953%
val Loss: 0.7863 Acc: 56.5337%
Epoch 100/250
train Loss: 0.5584 Acc: 69.1614%
val Loss: 0.8083 Acc: 55.9835%
Epoch 101/250
train Loss: 0.5644 Acc: 67.7508%
val Loss: 0.7435 Acc: 59.2847%
Epoch 102/250
train Loss: 0.5619 Acc: 68.2406%
val Loss: 0.7285 Acc: 59.9725%
Epoch 103/250
train Loss: 0.5553 Acc: 69.4357%
val Loss: 0.7335 Acc: 60.9354%
Epoch 104/250
train Loss: 0.5602 Acc: 68.9655%
val Loss: 0.7349 Acc: 60.2476%
Epoch 105/250
train Loss: 0.5537 Acc: 69.4553%
val Loss: 0.7299 Acc: 60.1100%
Epoch 106/250
train Loss: 0.5620 Acc: 68.9655%
val Loss: 0.7177 Acc: 60.5227%
Epoch 107/250
train Loss: 0.5601 Acc: 68.4757%
val Loss: 0.7707 Acc: 58.3219%
Epoch 108/250
train Loss: 0.5595 Acc: 68.4953%
val Loss: 0.7247 Acc: 58.5970%
Epoch 109/250
train Loss: 0.5571 Acc: 69.0439%
val Loss: 0.7421 Acc: 59.8349%
Epoch 110/250
train Loss: 0.5542 Acc: 69.5533%
val Loss: 0.7471 Acc: 58.7345%
Epoch 111/250
train Loss: 0.5525 Acc: 69.9060%
val Loss: 0.7483 Acc: 59.5598%
Epoch 112/250
train Loss: 0.5616 Acc: 68.1426%
val Loss: 0.7396 Acc: 60.3851%
Epoch 113/250
train Loss: 0.5511 Acc: 69.4945%
val Loss: 0.7021 Acc: 62.5860%
Epoch 114/250
train Loss: 0.5579 Acc: 68.7892%
val Loss: 0.7563 Acc: 60.2476%
Epoch 115/250
train Loss: 0.5571 Acc: 69.1614%
val Loss: 0.7383 Acc: 59.5598%
Epoch 116/250
train Loss: 0.5572 Acc: 69.5925%
val Loss: 0.6826 Acc: 62.1733%
Epoch 117/250
train Loss: 0.5551 Acc: 69.6317%
val Loss: 0.7155 Acc: 59.1472%
Epoch 118/250
train Loss: 0.5584 Acc: 68.6716%
val Loss: 0.6963 Acc: 61.7607%
Epoch 119/250
train Loss: 0.5507 Acc: 69.7688%
val Loss: 0.7470 Acc: 59.9725%
Epoch 120/250
train Loss: 0.5476 Acc: 70.0431%
val Loss: 0.6941 Acc: 62.1733%
Epoch 121/250
train Loss: 0.5445 Acc: 69.5141%
val Loss: 0.7891 Acc: 56.6713%
Epoch 122/250
train Loss: 0.5560 Acc: 69.2594%
val Loss: 0.7457 Acc: 59.4223%
Epoch 123/250
train Loss: 0.5531 Acc: 68.8480%
val Loss: 0.7990 Acc: 57.6341%
Epoch 124/250
train Loss: 0.5541 Acc: 69.0243%
val Loss: 0.7396 Acc: 61.2105%
Epoch 125/250
train Loss: 0.5432 Acc: 70.0823%
val Loss: 0.7151 Acc: 60.3851%
Epoch 126/250
train Loss: 0.5553 Acc: 69.1614%
val Loss: 0.7376 Acc: 59.8349%
Epoch 127/250
train Loss: 0.5487 Acc: 69.8276%
val Loss: 0.6788 Acc: 64.0990%
Epoch 128/250
train Loss: 0.5477 Acc: 69.9451%
val Loss: 0.7191 Acc: 59.9725%
Epoch 129/250
train Loss: 0.5451 Acc: 70.3762%
val Loss: 0.6915 Acc: 62.9986%
Epoch 130/250
train Loss: 0.5454 Acc: 69.1614%
val Loss: 0.7442 Acc: 58.0468%
Epoch 131/250
train Loss: 0.5555 Acc: 69.0243%
val Loss: 0.6870 Acc: 62.9986%
Epoch 132/250
train Loss: 0.5457 Acc: 70.3566%
val Loss: 0.7504 Acc: 58.0468%
Epoch 133/250
train Loss: 0.5441 Acc: 70.6701%
val Loss: 0.7568 Acc: 60.1100%
Epoch 134/250
train Loss: 0.5443 Acc: 70.0235%
val Loss: 0.7179 Acc: 60.5227%
Epoch 135/250
train Loss: 0.5468 Acc: 70.6309%
val Loss: 0.6931 Acc: 61.4856%
Epoch 136/250
train Loss: 0.5410 Acc: 69.8864%
val Loss: 0.7267 Acc: 57.7717%
Epoch 137/250
train Loss: 0.5451 Acc: 70.1607%
val Loss: 0.7363 Acc: 59.6974%
Epoch 138/250
train Loss: 0.5417 Acc: 70.4545%
val Loss: 0.7968 Acc: 57.0839%
Epoch 139/250
train Loss: 0.5376 Acc: 70.7288%
val Loss: 0.7328 Acc: 59.2847%
Epoch 140/250
train Loss: 0.5462 Acc: 70.0823%
val Loss: 0.7389 Acc: 59.2847%
Epoch 141/250
train Loss: 0.5377 Acc: 70.5133%
val Loss: 0.7139 Acc: 60.2476%
Epoch 142/250
train Loss: 0.5399 Acc: 70.4350%
val Loss: 0.7038 Acc: 61.8982%
Epoch 143/250
train Loss: 0.5398 Acc: 70.4154%
val Loss: 0.7181 Acc: 60.6602%
Epoch 144/250
train Loss: 0.5430 Acc: 69.9060%
val Loss: 0.6694 Acc: 65.7497%
Epoch 145/250
train Loss: 0.5313 Acc: 71.0423%
val Loss: 0.7412 Acc: 61.0729%
Epoch 146/250
train Loss: 0.5426 Acc: 69.6317%
val Loss: 0.7360 Acc: 63.2737%
Epoch 147/250
train Loss: 0.5321 Acc: 70.4937%
val Loss: 0.7644 Acc: 58.0468%
Epoch 148/250
train Loss: 0.5357 Acc: 70.2978%
val Loss: 0.7041 Acc: 63.5488%
Epoch 149/250
train Loss: 0.5338 Acc: 71.5125%
val Loss: 0.7519 Acc: 61.4856%
Epoch 150/250
train Loss: 0.5413 Acc: 70.6897%
val Loss: 0.7322 Acc: 63.1362%
Epoch 151/250
train Loss: 0.5297 Acc: 71.9632%
val Loss: 0.7609 Acc: 61.4856%
Epoch 152/250
train Loss: 0.5279 Acc: 71.3166%
val Loss: 0.7516 Acc: 60.9354%
Epoch 153/250
train Loss: 0.5289 Acc: 71.4342%
val Loss: 0.7394 Acc: 60.2476%
Epoch 154/250
train Loss: 0.5377 Acc: 70.9248%
val Loss: 0.7176 Acc: 60.5227%
Epoch 155/250
train Loss: 0.5309 Acc: 70.8660%
val Loss: 0.6980 Acc: 63.6864%
Epoch 156/250
train Loss: 0.5349 Acc: 69.7688%
val Loss: 0.6941 Acc: 64.0990%
Epoch 157/250
train Loss: 0.5322 Acc: 70.7092%
val Loss: 0.7587 Acc: 60.6602%
Epoch 158/250
train Loss: 0.5292 Acc: 70.3174%
val Loss: 0.8430 Acc: 55.2957%
Epoch 159/250
train Loss: 0.5255 Acc: 71.3950%
val Loss: 0.6843 Acc: 63.9615%
Epoch 160/250
train Loss: 0.5252 Acc: 71.0619%
val Loss: 0.7507 Acc: 61.3480%
Epoch 161/250
train Loss: 0.5293 Acc: 70.9444%
val Loss: 0.7724 Acc: 60.7978%
Epoch 162/250
train Loss: 0.5294 Acc: 71.1403%
val Loss: 0.6785 Acc: 65.8872%
Epoch 163/250
train Loss: 0.5233 Acc: 71.3362%
val Loss: 0.7502 Acc: 60.6602%
Epoch 164/250
train Loss: 0.5253 Acc: 71.9436%
val Loss: 0.7736 Acc: 59.5598%
Epoch 165/250
train Loss: 0.5157 Acc: 72.6685%
val Loss: 0.7544 Acc: 61.6231%
Epoch 166/250
train Loss: 0.5190 Acc: 72.3942%
val Loss: 0.6683 Acc: 66.4374%
Epoch 167/250
train Loss: 0.5297 Acc: 70.6897%
val Loss: 0.7303 Acc: 62.8611%
Epoch 168/250
train Loss: 0.5317 Acc: 70.7484%
val Loss: 0.7310 Acc: 61.3480%
Epoch 169/250
train Loss: 0.5177 Acc: 72.3354%
val Loss: 0.6554 Acc: 66.2999%
Epoch 170/250
train Loss: 0.5277 Acc: 71.9044%
val Loss: 0.6821 Acc: 62.8611%
Epoch 171/250
train Loss: 0.5240 Acc: 71.2774%
val Loss: 0.7059 Acc: 62.8611%
Epoch 172/250
train Loss: 0.5161 Acc: 72.1787%
val Loss: 0.7303 Acc: 62.1733%
Epoch 173/250
train Loss: 0.5218 Acc: 71.6301%
val Loss: 0.7247 Acc: 62.7235%
Epoch 174/250
train Loss: 0.5212 Acc: 71.6693%
val Loss: 0.7622 Acc: 62.1733%
Epoch 175/250
train Loss: 0.5273 Acc: 71.4929%
val Loss: 0.7039 Acc: 63.5488%
Epoch 176/250
train Loss: 0.5244 Acc: 71.6497%
val Loss: 0.7028 Acc: 64.3741%
Epoch 177/250
train Loss: 0.5265 Acc: 70.9444%
val Loss: 0.6604 Acc: 64.5117%
Epoch 178/250
train Loss: 0.5259 Acc: 70.8856%
val Loss: 0.6530 Acc: 66.9876%
Epoch 179/250
train Loss: 0.5175 Acc: 72.6293%
val Loss: 0.7482 Acc: 62.9986%
Epoch 180/250
train Loss: 0.5161 Acc: 72.6097%
val Loss: 0.6758 Acc: 66.4374%
Epoch 181/250
train Loss: 0.5186 Acc: 72.3550%
val Loss: 0.7615 Acc: 60.6602%
Epoch 182/250
train Loss: 0.5129 Acc: 72.5705%
val Loss: 0.6407 Acc: 67.1252%
Epoch 183/250
train Loss: 0.5138 Acc: 72.7273%
val Loss: 0.7019 Acc: 62.1733%
Epoch 184/250
train Loss: 0.5086 Acc: 72.1983%
val Loss: 0.7443 Acc: 62.4484%
Epoch 185/250
train Loss: 0.5131 Acc: 72.3550%
val Loss: 0.6608 Acc: 65.7497%
Epoch 186/250
train Loss: 0.5165 Acc: 72.2571%
val Loss: 0.6156 Acc: 69.0509%
Epoch 187/250
train Loss: 0.5113 Acc: 72.3550%
val Loss: 0.7045 Acc: 63.6864%
Epoch 188/250
train Loss: 0.5110 Acc: 72.1003%
val Loss: 0.6674 Acc: 68.6382%
Epoch 189/250
train Loss: 0.5221 Acc: 71.8848%
val Loss: 0.6900 Acc: 64.5117%
Epoch 190/250
train Loss: 0.5043 Acc: 73.0799%
val Loss: 0.6934 Acc: 64.0990%
Epoch 191/250
train Loss: 0.5141 Acc: 72.9428%
val Loss: 0.6707 Acc: 64.9243%
Epoch 192/250
train Loss: 0.5135 Acc: 72.7273%
val Loss: 0.6926 Acc: 63.4113%
Epoch 193/250
train Loss: 0.5124 Acc: 72.5901%
val Loss: 0.7608 Acc: 61.4856%
Epoch 194/250
train Loss: 0.5182 Acc: 72.2375%
val Loss: 0.7984 Acc: 57.3590%
Epoch 195/250
train Loss: 0.5115 Acc: 72.6489%
val Loss: 0.7072 Acc: 62.4484%
Epoch 196/250
train Loss: 0.5105 Acc: 72.7861%
val Loss: 0.7083 Acc: 63.4113%
Epoch 197/250
train Loss: 0.5103 Acc: 72.9624%
val Loss: 0.6901 Acc: 61.4856%
Epoch 198/250
train Loss: 0.5073 Acc: 72.3942%
val Loss: 0.6543 Acc: 63.9615%
Epoch 199/250
train Loss: 0.5196 Acc: 72.1591%
val Loss: 0.7285 Acc: 61.3480%
Epoch 200/250
train Loss: 0.5146 Acc: 72.7665%
val Loss: 0.7044 Acc: 62.9986%
Epoch 201/250
train Loss: 0.5034 Acc: 73.0995%
val Loss: 0.6619 Acc: 65.7497%
Epoch 202/250
train Loss: 0.5152 Acc: 71.9828%
val Loss: 0.7010 Acc: 63.9615%
Epoch 203/250
train Loss: 0.5075 Acc: 72.6293%
val Loss: 0.6873 Acc: 65.3370%
Epoch 204/250
train Loss: 0.5061 Acc: 73.3934%
val Loss: 0.7310 Acc: 65.1994%
Epoch 205/250
train Loss: 0.4984 Acc: 73.9028%
val Loss: 0.7169 Acc: 64.7868%
Epoch 206/250
train Loss: 0.5050 Acc: 73.4130%
val Loss: 0.6960 Acc: 64.0990%
Epoch 207/250
train Loss: 0.5082 Acc: 72.8448%
val Loss: 0.7691 Acc: 61.7607%
Epoch 208/250
train Loss: 0.5033 Acc: 74.0008%
val Loss: 0.7034 Acc: 62.3109%
Epoch 209/250
train Loss: 0.5018 Acc: 73.2955%
val Loss: 0.6743 Acc: 67.8129%
Epoch 210/250
train Loss: 0.5065 Acc: 73.3542%
val Loss: 0.6748 Acc: 64.5117%
Epoch 211/250
train Loss: 0.5093 Acc: 72.4334%
val Loss: 0.6890 Acc: 66.2999%
Epoch 212/250
train Loss: 0.5037 Acc: 73.4326%
val Loss: 0.6674 Acc: 66.1623%
Epoch 213/250
train Loss: 0.5020 Acc: 72.9624%
val Loss: 0.7277 Acc: 62.1733%
Epoch 214/250
train Loss: 0.5004 Acc: 73.1583%
val Loss: 0.7124 Acc: 64.9243%
Epoch 215/250
train Loss: 0.4999 Acc: 72.9624%
val Loss: 0.7314 Acc: 63.9615%
Epoch 216/250
train Loss: 0.5052 Acc: 73.3150%
val Loss: 0.7125 Acc: 64.0990%
Epoch 217/250
train Loss: 0.5097 Acc: 72.7077%
val Loss: 0.6379 Acc: 69.7387%
Epoch 218/250
train Loss: 0.5007 Acc: 73.6873%
val Loss: 0.6566 Acc: 65.4746%
Epoch 219/250
train Loss: 0.4954 Acc: 73.0995%
val Loss: 0.6421 Acc: 68.2256%
Epoch 220/250
train Loss: 0.4972 Acc: 74.5298%
val Loss: 0.6520 Acc: 65.7497%
Epoch 221/250
train Loss: 0.4984 Acc: 73.5697%
val Loss: 0.6402 Acc: 68.5007%
Epoch 222/250
train Loss: 0.5092 Acc: 72.0024%
val Loss: 0.8216 Acc: 61.4856%
Epoch 223/250
train Loss: 0.4956 Acc: 73.1779%
val Loss: 0.7061 Acc: 64.7868%
Epoch 224/250
train Loss: 0.4907 Acc: 73.9616%
val Loss: 0.7097 Acc: 63.9615%
Epoch 225/250
train Loss: 0.4923 Acc: 74.4710%
val Loss: 0.6718 Acc: 66.7125%
Epoch 226/250
train Loss: 0.4961 Acc: 74.1575%
val Loss: 0.7879 Acc: 63.6864%
Epoch 227/250
train Loss: 0.4951 Acc: 73.9420%
val Loss: 0.7526 Acc: 63.6864%
Epoch 228/250
train Loss: 0.4923 Acc: 73.8440%
val Loss: 0.7084 Acc: 66.2999%
Epoch 229/250
train Loss: 0.4913 Acc: 73.8245%
val Loss: 0.6853 Acc: 67.1252%
Epoch 230/250
train Loss: 0.4939 Acc: 73.5697%
val Loss: 0.6804 Acc: 67.5378%
Epoch 231/250
train Loss: 0.4853 Acc: 74.7453%
val Loss: 0.7595 Acc: 63.9615%
Epoch 232/250
train Loss: 0.5000 Acc: 73.7069%
val Loss: 0.7275 Acc: 67.1252%
Epoch 233/250
train Loss: 0.4904 Acc: 74.1771%
val Loss: 0.7491 Acc: 64.3741%
Epoch 234/250
train Loss: 0.4982 Acc: 73.6285%
val Loss: 0.7518 Acc: 64.9243%
Epoch 235/250
train Loss: 0.4914 Acc: 73.6481%
val Loss: 0.7519 Acc: 63.8239%
Epoch 236/250
train Loss: 0.4998 Acc: 73.4914%
val Loss: 0.6931 Acc: 65.8872%
Epoch 237/250
train Loss: 0.4988 Acc: 73.8636%
val Loss: 0.7341 Acc: 64.5117%
Epoch 238/250
train Loss: 0.4948 Acc: 73.4914%
val Loss: 0.7310 Acc: 65.3370%
Epoch 239/250
train Loss: 0.4883 Acc: 74.5102%
val Loss: 0.6657 Acc: 66.9876%
Epoch 240/250
train Loss: 0.4948 Acc: 73.5893%
val Loss: 0.7157 Acc: 64.2366%
Epoch 241/250
train Loss: 0.4977 Acc: 74.0596%
val Loss: 0.7495 Acc: 63.4113%
Epoch 242/250
train Loss: 0.4806 Acc: 74.8433%
val Loss: 0.8384 Acc: 58.7345%
Epoch 243/250
train Loss: 0.4909 Acc: 74.5886%
val Loss: 0.6619 Acc: 66.2999%
Epoch 244/250
train Loss: 0.4880 Acc: 73.7657%
val Loss: 0.7293 Acc: 64.2366%
Epoch 245/250
train Loss: 0.4883 Acc: 74.7649%
val Loss: 0.7096 Acc: 65.1994%
Epoch 246/250
train Loss: 0.4860 Acc: 74.5298%
val Loss: 0.7490 Acc: 62.9986%
Epoch 247/250
train Loss: 0.4874 Acc: 74.4514%
val Loss: 0.7723 Acc: 63.4113%
Epoch 248/250
train Loss: 0.4882 Acc: 74.5690%
val Loss: 0.7223 Acc: 63.5488%
Epoch 249/250
train Loss: 0.4963 Acc: 73.7657%
val Loss: 0.7091 Acc: 65.1994%
Epoch 250/250
train Loss: 0.4852 Acc: 74.5690%
val Loss: 0.7146 Acc: 66.1623%
[0.6847036634119327, 0.6658651023449195, 0.658306623327321, 0.6483092982567216, 0.6422359534191862, 0.6438761078825564, 0.6359769855173404, 0.6324707906821678, 0.6338422042449067, 0.6312030589318948, 0.6302467814433538, 0.628569130426664, 0.6246095647258818, 0.6206526040657186, 0.6203848921393152, 0.6195760365563874, 0.6178593114252001, 0.620958988008828, 0.6156050301270799, 0.617367553299871, 0.6184135916464755, 0.6172419706108428, 0.6114728338666097, 0.6147527087444796, 0.611129697213726, 0.6138813929124312, 0.6100087317179737, 0.6083306823404605, 0.6030676045387889, 0.6044458408714461, 0.6036478882673019, 0.6020548091413085, 0.6048559856639013, 0.5996517692240054, 0.609831173218156, 0.5987277113158127, 0.6016053338783288, 0.6004774671363232, 0.599829253171305, 0.5970391522754322, 0.5959640003297022, 0.5969188798183932, 0.597813950997535, 0.5875630907504162, 0.6012129288482068, 0.5958859957871392, 0.589828567258243, 0.5907112321696685, 0.5908111167551956, 0.5938382911084213, 0.5915191743441136, 0.5892366686584808, 0.5908749367002409, 0.5866017618149425, 0.5877890185128932, 0.5817579279498994, 0.5891699103352418, 0.5836831120114342, 0.5843772039891784, 0.5834726093331101, 0.5807471350068957, 0.588796695945405, 0.5887640462400023, 0.5812497309009109, 0.5746815916512827, 0.5795767432469933, 0.5736444112275462, 0.5778314463770875, 0.5758863756649173, 0.5778800179965817, 0.5783742550398489, 0.5754186227030141, 0.5837191274173581, 0.5717178652279056, 0.5764774631557046, 0.5749705495505497, 0.5754426925159921, 0.5771085653559167, 0.5712882852105885, 0.5742164452247859, 0.5668911147267094, 0.5691858481463967, 0.5701890911988704, 0.5720323537210685, 0.5682723561424446, 0.5692314490629214, 0.5712720695707866, 0.5669109953981956, 0.5704422936917846, 0.5710866032722975, 0.5737981248798789, 0.5705161182484283, 0.5658363871813568, 0.5566751054461847, 0.5642442738748269, 0.564844822995715, 0.5589426884830558, 0.5629392737131508, 0.5606289451772516, 0.5583617971234934, 0.5644425493796418, 0.5619244349413904, 0.555311305956407, 0.5602183007521315, 0.5536560852512671, 0.561993913964418, 0.5601404501726635, 0.5594738411679163, 0.557052743192003, 0.5542177045046349, 0.5525076535054508, 0.5615715223793699, 0.5511435694455353, 0.5578598384760016, 0.5570705940357197, 0.5572309039976904, 0.5551348422948843, 0.558373792791815, 0.5506798953845583, 0.5476350223756509, 0.5445057527398615, 0.5559740176768886, 0.5530723038138267, 0.554143434968488, 0.543154479381059, 0.5552662569901039, 0.5487347069578858, 0.5476894998064609, 0.5451148905350497, 0.5453656691368844, 0.5554901772531969, 0.545705011085283, 0.544094432110323, 0.5442571184104513, 0.5468321453814969, 0.5410461485572743, 0.5450897592362192, 0.5417165888887961, 0.5375996190926124, 0.546217242285002, 0.5376626999587475, 0.5398966780276881, 0.5397732276527859, 0.5430468246107191, 0.5312810500961113, 0.542621727834301, 0.5321012891385248, 0.5356936437956592, 0.5338015337722802, 0.5413436854147239, 0.529729080704686, 0.5279382256131188, 0.528940722860139, 0.5376599379654589, 0.530850328434971, 0.5348629882343137, 0.5322448441227402, 0.5292283752495218, 0.5254796199664054, 0.525223688458948, 0.5293289800422692, 0.5293978323757088, 0.5232866460626776, 0.525303665560241, 0.5156609587908538, 0.5190025267182473, 0.5296685546169461, 0.531650780884076, 0.5177105046738651, 0.5276699776186091, 0.5240006337532055, 0.5160768565526203, 0.521754739800217, 0.5212472453012735, 0.5272862227359162, 0.5244314070779328, 0.5264783093361272, 0.5259278667010484, 0.5175162141786473, 0.5160986162652043, 0.5186428409011387, 0.5129300518469377, 0.5138084621265017, 0.5085920794256802, 0.5131320902920068, 0.5165114328031629, 0.5112526484978236, 0.5110410734030146, 0.5221194080237684, 0.5042996827898354, 0.5141235639495909, 0.5134534157555679, 0.5123540742270252, 0.5181667382440597, 0.5114993139120478, 0.5105299764666064, 0.5102756633467062, 0.5072649112876306, 0.5195758282017184, 0.5145766590269382, 0.5033936182907008, 0.5151810508723543, 0.5074545426241657, 0.5060785409051423, 0.498376731588549, 0.5050150541863098, 0.5082185944793366, 0.5033263024117879, 0.5018193500355868, 0.506460772973243, 0.5092695009745775, 0.5036613505835817, 0.5019642381832518, 0.5004092903346478, 0.4999201591112023, 0.5051990979143819, 0.5097182503314601, 0.5006541499523534, 0.49540169083960006, 0.4971979180099822, 0.4983540052156837, 0.5092257465875261, 0.4955696842887185, 0.49069516841894406, 0.49231597902632807, 0.4961066087211561, 0.49514571346085645, 0.4922603081012594, 0.4913065427896745, 0.4938866514770962, 0.4853093820305827, 0.500037771594188, 0.49043893029323565, 0.498234389250555, 0.4914096389257796, 0.4997807926145093, 0.49878049774977107, 0.49478446848713864, 0.48832928106702606, 0.49478936868027834, 0.4977005653807362, 0.4806182214458908, 0.49091407768778667, 0.4880449865305312, 0.4882901471610353, 0.4859907074034401, 0.48740138062115373, 0.48818064931791777, 0.4962948899844597, 0.485162376049544]
[tensor(56.8378, dtype=torch.float64), tensor(59.2085, dtype=torch.float64), tensor(59.6003, dtype=torch.float64), tensor(59.7962, dtype=torch.float64), tensor(61.5204, dtype=torch.float64), tensor(60.2861, dtype=torch.float64), tensor(62.5392, dtype=torch.float64), tensor(62.2453, dtype=torch.float64), tensor(61.5400, dtype=torch.float64), tensor(62.4216, dtype=torch.float64), tensor(62.0102, dtype=torch.float64), tensor(62.2845, dtype=torch.float64), tensor(62.6959, dtype=torch.float64), tensor(62.9310, dtype=torch.float64), tensor(63.5776, dtype=torch.float64), tensor(63.1270, dtype=torch.float64), tensor(62.7939, dtype=torch.float64), tensor(62.8135, dtype=torch.float64), tensor(63.8127, dtype=torch.float64), tensor(64.4397, dtype=torch.float64), tensor(63.7931, dtype=torch.float64), tensor(63.4600, dtype=torch.float64), tensor(64.2437, dtype=torch.float64), tensor(64.3417, dtype=torch.float64), tensor(63.3229, dtype=torch.float64), tensor(63.6560, dtype=torch.float64), tensor(64.5572, dtype=torch.float64), tensor(64.0086, dtype=torch.float64), tensor(64.6160, dtype=torch.float64), tensor(64.9687, dtype=torch.float64), tensor(65.3409, dtype=torch.float64), tensor(65.7328, dtype=torch.float64), tensor(64.8511, dtype=torch.float64), tensor(66.3205, dtype=torch.float64), tensor(65.2625, dtype=torch.float64), tensor(66.0658, dtype=torch.float64), tensor(64.8707, dtype=torch.float64), tensor(65.3801, dtype=torch.float64), tensor(64.9491, dtype=torch.float64), tensor(65.8111, dtype=torch.float64), tensor(66.3009, dtype=torch.float64), tensor(65.7132, dtype=torch.float64), tensor(65.8895, dtype=torch.float64), tensor(67.0455, dtype=torch.float64), tensor(64.8903, dtype=torch.float64), tensor(66.2422, dtype=torch.float64), tensor(67.0846, dtype=torch.float64), tensor(66.4773, dtype=torch.float64), tensor(66.2226, dtype=torch.float64), tensor(66.4381, dtype=torch.float64), tensor(65.9875, dtype=torch.float64), tensor(67.0259, dtype=torch.float64), tensor(67.0846, dtype=torch.float64), tensor(66.2813, dtype=torch.float64), tensor(66.7124, dtype=torch.float64), tensor(68.0251, dtype=torch.float64), tensor(66.6144, dtype=torch.float64), tensor(67.5157, dtype=torch.float64), tensor(66.0266, dtype=torch.float64), tensor(66.4773, dtype=torch.float64), tensor(67.3589, dtype=torch.float64), tensor(66.1246, dtype=torch.float64), tensor(66.3401, dtype=torch.float64), tensor(67.0455, dtype=torch.float64), tensor(67.0455, dtype=torch.float64), tensor(67.2022, dtype=torch.float64), tensor(67.6724, dtype=torch.float64), tensor(67.6332, dtype=torch.float64), tensor(66.9867, dtype=torch.float64), tensor(67.1826, dtype=torch.float64), tensor(67.3589, dtype=torch.float64), tensor(67.9271, dtype=torch.float64), tensor(66.9671, dtype=torch.float64), tensor(67.8096, dtype=torch.float64), tensor(67.0455, dtype=torch.float64), tensor(68.3386, dtype=torch.float64), tensor(67.7116, dtype=torch.float64), tensor(67.7312, dtype=torch.float64), tensor(67.2414, dtype=torch.float64), tensor(67.0259, dtype=torch.float64), tensor(68.0447, dtype=torch.float64), tensor(67.8096, dtype=torch.float64), tensor(67.7704, dtype=torch.float64), tensor(67.7312, dtype=torch.float64), tensor(68.1426, dtype=torch.float64), tensor(67.7900, dtype=torch.float64), tensor(67.7900, dtype=torch.float64), tensor(68.7696, dtype=torch.float64), tensor(68.5149, dtype=torch.float64), tensor(67.4961, dtype=torch.float64), tensor(67.9859, dtype=torch.float64), tensor(68.3386, dtype=torch.float64), tensor(68.6520, dtype=torch.float64), tensor(68.7892, dtype=torch.float64), tensor(69.2398, dtype=torch.float64), tensor(67.8683, dtype=torch.float64), tensor(68.9851, dtype=torch.float64), tensor(68.7696, dtype=torch.float64), tensor(68.4953, dtype=torch.float64), tensor(69.1614, dtype=torch.float64), tensor(67.7508, dtype=torch.float64), tensor(68.2406, dtype=torch.float64), tensor(69.4357, dtype=torch.float64), tensor(68.9655, dtype=torch.float64), tensor(69.4553, dtype=torch.float64), tensor(68.9655, dtype=torch.float64), tensor(68.4757, dtype=torch.float64), tensor(68.4953, dtype=torch.float64), tensor(69.0439, dtype=torch.float64), tensor(69.5533, dtype=torch.float64), tensor(69.9060, dtype=torch.float64), tensor(68.1426, dtype=torch.float64), tensor(69.4945, dtype=torch.float64), tensor(68.7892, dtype=torch.float64), tensor(69.1614, dtype=torch.float64), tensor(69.5925, dtype=torch.float64), tensor(69.6317, dtype=torch.float64), tensor(68.6716, dtype=torch.float64), tensor(69.7688, dtype=torch.float64), tensor(70.0431, dtype=torch.float64), tensor(69.5141, dtype=torch.float64), tensor(69.2594, dtype=torch.float64), tensor(68.8480, dtype=torch.float64), tensor(69.0243, dtype=torch.float64), tensor(70.0823, dtype=torch.float64), tensor(69.1614, dtype=torch.float64), tensor(69.8276, dtype=torch.float64), tensor(69.9451, dtype=torch.float64), tensor(70.3762, dtype=torch.float64), tensor(69.1614, dtype=torch.float64), tensor(69.0243, dtype=torch.float64), tensor(70.3566, dtype=torch.float64), tensor(70.6701, dtype=torch.float64), tensor(70.0235, dtype=torch.float64), tensor(70.6309, dtype=torch.float64), tensor(69.8864, dtype=torch.float64), tensor(70.1607, dtype=torch.float64), tensor(70.4545, dtype=torch.float64), tensor(70.7288, dtype=torch.float64), tensor(70.0823, dtype=torch.float64), tensor(70.5133, dtype=torch.float64), tensor(70.4350, dtype=torch.float64), tensor(70.4154, dtype=torch.float64), tensor(69.9060, dtype=torch.float64), tensor(71.0423, dtype=torch.float64), tensor(69.6317, dtype=torch.float64), tensor(70.4937, dtype=torch.float64), tensor(70.2978, dtype=torch.float64), tensor(71.5125, dtype=torch.float64), tensor(70.6897, dtype=torch.float64), tensor(71.9632, dtype=torch.float64), tensor(71.3166, dtype=torch.float64), tensor(71.4342, dtype=torch.float64), tensor(70.9248, dtype=torch.float64), tensor(70.8660, dtype=torch.float64), tensor(69.7688, dtype=torch.float64), tensor(70.7092, dtype=torch.float64), tensor(70.3174, dtype=torch.float64), tensor(71.3950, dtype=torch.float64), tensor(71.0619, dtype=torch.float64), tensor(70.9444, dtype=torch.float64), tensor(71.1403, dtype=torch.float64), tensor(71.3362, dtype=torch.float64), tensor(71.9436, dtype=torch.float64), tensor(72.6685, dtype=torch.float64), tensor(72.3942, dtype=torch.float64), tensor(70.6897, dtype=torch.float64), tensor(70.7484, dtype=torch.float64), tensor(72.3354, dtype=torch.float64), tensor(71.9044, dtype=torch.float64), tensor(71.2774, dtype=torch.float64), tensor(72.1787, dtype=torch.float64), tensor(71.6301, dtype=torch.float64), tensor(71.6693, dtype=torch.float64), tensor(71.4929, dtype=torch.float64), tensor(71.6497, dtype=torch.float64), tensor(70.9444, dtype=torch.float64), tensor(70.8856, dtype=torch.float64), tensor(72.6293, dtype=torch.float64), tensor(72.6097, dtype=torch.float64), tensor(72.3550, dtype=torch.float64), tensor(72.5705, dtype=torch.float64), tensor(72.7273, dtype=torch.float64), tensor(72.1983, dtype=torch.float64), tensor(72.3550, dtype=torch.float64), tensor(72.2571, dtype=torch.float64), tensor(72.3550, dtype=torch.float64), tensor(72.1003, dtype=torch.float64), tensor(71.8848, dtype=torch.float64), tensor(73.0799, dtype=torch.float64), tensor(72.9428, dtype=torch.float64), tensor(72.7273, dtype=torch.float64), tensor(72.5901, dtype=torch.float64), tensor(72.2375, dtype=torch.float64), tensor(72.6489, dtype=torch.float64), tensor(72.7861, dtype=torch.float64), tensor(72.9624, dtype=torch.float64), tensor(72.3942, dtype=torch.float64), tensor(72.1591, dtype=torch.float64), tensor(72.7665, dtype=torch.float64), tensor(73.0995, dtype=torch.float64), tensor(71.9828, dtype=torch.float64), tensor(72.6293, dtype=torch.float64), tensor(73.3934, dtype=torch.float64), tensor(73.9028, dtype=torch.float64), tensor(73.4130, dtype=torch.float64), tensor(72.8448, dtype=torch.float64), tensor(74.0008, dtype=torch.float64), tensor(73.2955, dtype=torch.float64), tensor(73.3542, dtype=torch.float64), tensor(72.4334, dtype=torch.float64), tensor(73.4326, dtype=torch.float64), tensor(72.9624, dtype=torch.float64), tensor(73.1583, dtype=torch.float64), tensor(72.9624, dtype=torch.float64), tensor(73.3150, dtype=torch.float64), tensor(72.7077, dtype=torch.float64), tensor(73.6873, dtype=torch.float64), tensor(73.0995, dtype=torch.float64), tensor(74.5298, dtype=torch.float64), tensor(73.5697, dtype=torch.float64), tensor(72.0024, dtype=torch.float64), tensor(73.1779, dtype=torch.float64), tensor(73.9616, dtype=torch.float64), tensor(74.4710, dtype=torch.float64), tensor(74.1575, dtype=torch.float64), tensor(73.9420, dtype=torch.float64), tensor(73.8440, dtype=torch.float64), tensor(73.8245, dtype=torch.float64), tensor(73.5697, dtype=torch.float64), tensor(74.7453, dtype=torch.float64), tensor(73.7069, dtype=torch.float64), tensor(74.1771, dtype=torch.float64), tensor(73.6285, dtype=torch.float64), tensor(73.6481, dtype=torch.float64), tensor(73.4914, dtype=torch.float64), tensor(73.8636, dtype=torch.float64), tensor(73.4914, dtype=torch.float64), tensor(74.5102, dtype=torch.float64), tensor(73.5893, dtype=torch.float64), tensor(74.0596, dtype=torch.float64), tensor(74.8433, dtype=torch.float64), tensor(74.5886, dtype=torch.float64), tensor(73.7657, dtype=torch.float64), tensor(74.7649, dtype=torch.float64), tensor(74.5298, dtype=torch.float64), tensor(74.4514, dtype=torch.float64), tensor(74.5690, dtype=torch.float64), tensor(73.7657, dtype=torch.float64), tensor(74.5690, dtype=torch.float64)]
[0.7364947888513379, 0.7526793549608496, 0.8005839190260088, 0.8071457095126502, 0.8443100937786916, 0.8136718606030596, 0.797723863770906, 0.8069523039185525, 0.7954030587059924, 0.7932665920323158, 0.798617301239764, 0.8050227324277204, 0.8200117970267221, 0.826150098807874, 0.8327296325396311, 0.8222932564671135, 0.8106856159199711, 0.7894768388596001, 0.8203190440488812, 0.7983802602904371, 0.7824552912659626, 0.797119421922849, 0.7856457333453256, 0.7929316322937956, 0.7524827071036401, 0.7559889141762273, 0.7866318681709048, 0.7872899457545851, 0.8171668903699767, 0.775661697331944, 0.7839549324863386, 0.7956785261221077, 0.7876546099228577, 0.7613643657882407, 0.7878310656613136, 0.7919834004471685, 0.7808282862339256, 0.7776869445260322, 0.8016972840242241, 0.7962798597560132, 0.7762149610086517, 0.7711446614836073, 0.7509003180749151, 0.8073907773301231, 0.770608946742513, 0.7952436007856503, 0.7966353558772532, 0.7591262574536928, 0.7874467336000078, 0.7777921639413584, 0.7786513331667757, 0.745661255745973, 0.7857660375730208, 0.8044969381295667, 0.7777054947704856, 0.7581517361053246, 0.7536002740243443, 0.8029169945310202, 0.7425057234593566, 0.786176341927855, 0.7289454001508028, 0.7431888368795466, 0.7541065682571233, 0.7609138907723775, 0.8123255293339778, 0.7850965943592286, 0.7758672557965925, 0.7680755499468545, 0.7587409753255044, 0.752953242976859, 0.7632380175459336, 0.770222052993276, 0.7507242652212245, 0.7308536251440665, 0.7669191990134805, 0.8002998476343273, 0.738188876984372, 0.7616037840364396, 0.7347087869945877, 0.7451584585923112, 0.7225515114392014, 0.7110783925082693, 0.7450561520977886, 0.7677297176488328, 0.7181869388610493, 0.7373105283452523, 0.7550078608310862, 0.7644444536802038, 0.7655644693925587, 0.7754918199949776, 0.7427583074471318, 0.7291265373544811, 0.7342925856333159, 0.7376117462797047, 0.7233392854504277, 0.6967335764774758, 0.7633492022629781, 0.7491571508870656, 0.7862936238639129, 0.8083472739715524, 0.7434885692071718, 0.7285247437727664, 0.733481716257014, 0.7348663399439238, 0.7298512394031436, 0.7176501551717315, 0.7707131929705855, 0.7246777491523606, 0.7421060602963381, 0.7471249783219465, 0.7483261804797134, 0.7396210719007573, 0.7021022709560526, 0.7563417261207612, 0.7382794794059029, 0.6825808544106464, 0.7154972249900786, 0.6962854216974408, 0.7469957179839543, 0.694112398109541, 0.789090499015425, 0.7456602287751608, 0.7989572111153374, 0.7396284912442571, 0.7150931607578283, 0.7375561277672874, 0.6787994583502432, 0.7191467769700198, 0.6914785463347559, 0.7441872624780321, 0.6869825153436097, 0.7504144790575954, 0.7568272522751862, 0.7178564954656682, 0.6931273389551301, 0.7267027537465259, 0.7362811245649357, 0.7967615660479833, 0.7327622681077277, 0.738891154732632, 0.7139471890673841, 0.7038343939853338, 0.7181003299507183, 0.6693976483777924, 0.7412197523464691, 0.7359582806387827, 0.7644008063548533, 0.7041493227098142, 0.7519463822962985, 0.7322135655391331, 0.7608557008155602, 0.7515906283255293, 0.7394388538607213, 0.717644046638494, 0.6980240884476547, 0.694102283931008, 0.7587220490060778, 0.8430199438636864, 0.6842533854211705, 0.7507424632489599, 0.7723742272535741, 0.6784889481582537, 0.750229238070025, 0.7736141196307322, 0.7544141037100119, 0.6682709937574447, 0.7302537399768173, 0.7310172066071995, 0.6554244184428429, 0.6820665993093624, 0.7058936646092873, 0.7302968415138318, 0.7246551302145076, 0.7621887725517871, 0.7038692896762937, 0.7028087187338072, 0.6603813350118666, 0.6530068492954993, 0.748188674695882, 0.6758347080695088, 0.7614773242968804, 0.6406526363863414, 0.7019264984983525, 0.7442590095809926, 0.6608236382883221, 0.6155875501475275, 0.704527640441097, 0.6673848425669716, 0.6900312782481743, 0.6933901368997776, 0.670712346059912, 0.6926400192502112, 0.7608396878268728, 0.7984034770784548, 0.7071821145211158, 0.7083494159017665, 0.6900747024045522, 0.6542744508635719, 0.7285122537301364, 0.7043984092905236, 0.6619366796833285, 0.7010127304344754, 0.6873395564303602, 0.7309552467180905, 0.7169036319200077, 0.6959971146865593, 0.7691444875121608, 0.7033545407665153, 0.6743042686290557, 0.6748178811971882, 0.6890134624060771, 0.667425912134913, 0.7277344521826858, 0.7124056488972419, 0.7313668080996286, 0.7124602371593617, 0.6379014954606309, 0.6565501308670726, 0.6421420611082441, 0.6520263282928047, 0.6402403063754432, 0.8216187068487952, 0.7060793355373766, 0.7097141611526888, 0.6717867178142972, 0.7879298803238954, 0.7525564423945452, 0.7084344222096334, 0.685318257326601, 0.6803603743752554, 0.7595371212477205, 0.7274647812567846, 0.7491189263545828, 0.7517935260617913, 0.7519351951521726, 0.693073901874327, 0.7341011787215158, 0.7309772277960587, 0.6656630770540303, 0.7157200474850577, 0.7494943195228891, 0.838439719221123, 0.6619453596609659, 0.7293278182225181, 0.7096262028981436, 0.7489831216397607, 0.7723329308272392, 0.722262353155931, 0.7090929195674283, 0.7146112298211172]
[tensor(50.2063, dtype=torch.float64), tensor(48.5557, dtype=torch.float64), tensor(46.2173, dtype=torch.float64), tensor(39.8900, dtype=torch.float64), tensor(42.3659, dtype=torch.float64), tensor(39.0646, dtype=torch.float64), tensor(43.1912, dtype=torch.float64), tensor(45.5296, dtype=torch.float64), tensor(48.0055, dtype=torch.float64), tensor(42.6410, dtype=torch.float64), tensor(48.2806, dtype=torch.float64), tensor(51.4443, dtype=torch.float64), tensor(43.3287, dtype=torch.float64), tensor(48.6933, dtype=torch.float64), tensor(52.4072, dtype=torch.float64), tensor(41.8157, dtype=torch.float64), tensor(44.9794, dtype=torch.float64), tensor(49.2435, dtype=torch.float64), tensor(53.0949, dtype=torch.float64), tensor(39.6149, dtype=torch.float64), tensor(49.1059, dtype=torch.float64), tensor(46.3549, dtype=torch.float64), tensor(53.7827, dtype=torch.float64), tensor(54.6080, dtype=torch.float64), tensor(52.5447, dtype=torch.float64), tensor(51.5818, dtype=torch.float64), tensor(48.5557, dtype=torch.float64), tensor(47.0426, dtype=torch.float64), tensor(54.1953, dtype=torch.float64), tensor(50.7565, dtype=torch.float64), tensor(50.2063, dtype=torch.float64), tensor(45.5296, dtype=torch.float64), tensor(48.0055, dtype=torch.float64), tensor(56.9464, dtype=torch.float64), tensor(48.4182, dtype=torch.float64), tensor(52.8198, dtype=torch.float64), tensor(57.9092, dtype=torch.float64), tensor(49.9312, dtype=torch.float64), tensor(51.9945, dtype=torch.float64), tensor(53.2325, dtype=torch.float64), tensor(49.9312, dtype=torch.float64), tensor(52.6823, dtype=torch.float64), tensor(53.6451, dtype=torch.float64), tensor(54.8831, dtype=torch.float64), tensor(53.0949, dtype=torch.float64), tensor(52.8198, dtype=torch.float64), tensor(54.3329, dtype=torch.float64), tensor(52.5447, dtype=torch.float64), tensor(54.7455, dtype=torch.float64), tensor(55.5708, dtype=torch.float64), tensor(53.5076, dtype=torch.float64), tensor(52.9574, dtype=torch.float64), tensor(52.8198, dtype=torch.float64), tensor(52.8198, dtype=torch.float64), tensor(53.2325, dtype=torch.float64), tensor(53.2325, dtype=torch.float64), tensor(54.8831, dtype=torch.float64), tensor(52.2696, dtype=torch.float64), tensor(56.2586, dtype=torch.float64), tensor(53.6451, dtype=torch.float64), tensor(57.3590, dtype=torch.float64), tensor(56.5337, dtype=torch.float64), tensor(55.8459, dtype=torch.float64), tensor(54.7455, dtype=torch.float64), tensor(54.6080, dtype=torch.float64), tensor(54.4704, dtype=torch.float64), tensor(53.6451, dtype=torch.float64), tensor(54.7455, dtype=torch.float64), tensor(54.8831, dtype=torch.float64), tensor(55.4333, dtype=torch.float64), tensor(57.0839, dtype=torch.float64), tensor(55.4333, dtype=torch.float64), tensor(54.8831, dtype=torch.float64), tensor(55.1582, dtype=torch.float64), tensor(55.1582, dtype=torch.float64), tensor(54.4704, dtype=torch.float64), tensor(55.5708, dtype=torch.float64), tensor(55.9835, dtype=torch.float64), tensor(57.4966, dtype=torch.float64), tensor(59.6974, dtype=torch.float64), tensor(57.2215, dtype=torch.float64), tensor(58.3219, dtype=torch.float64), tensor(59.4223, dtype=torch.float64), tensor(57.4966, dtype=torch.float64), tensor(59.2847, dtype=torch.float64), tensor(55.8459, dtype=torch.float64), tensor(58.5970, dtype=torch.float64), tensor(57.4966, dtype=torch.float64), tensor(58.5970, dtype=torch.float64), tensor(56.1210, dtype=torch.float64), tensor(57.2215, dtype=torch.float64), tensor(57.6341, dtype=torch.float64), tensor(60.2476, dtype=torch.float64), tensor(57.9092, dtype=torch.float64), tensor(60.5227, dtype=torch.float64), tensor(61.6231, dtype=torch.float64), tensor(58.1843, dtype=torch.float64), tensor(57.3590, dtype=torch.float64), tensor(56.5337, dtype=torch.float64), tensor(55.9835, dtype=torch.float64), tensor(59.2847, dtype=torch.float64), tensor(59.9725, dtype=torch.float64), tensor(60.9354, dtype=torch.float64), tensor(60.2476, dtype=torch.float64), tensor(60.1100, dtype=torch.float64), tensor(60.5227, dtype=torch.float64), tensor(58.3219, dtype=torch.float64), tensor(58.5970, dtype=torch.float64), tensor(59.8349, dtype=torch.float64), tensor(58.7345, dtype=torch.float64), tensor(59.5598, dtype=torch.float64), tensor(60.3851, dtype=torch.float64), tensor(62.5860, dtype=torch.float64), tensor(60.2476, dtype=torch.float64), tensor(59.5598, dtype=torch.float64), tensor(62.1733, dtype=torch.float64), tensor(59.1472, dtype=torch.float64), tensor(61.7607, dtype=torch.float64), tensor(59.9725, dtype=torch.float64), tensor(62.1733, dtype=torch.float64), tensor(56.6713, dtype=torch.float64), tensor(59.4223, dtype=torch.float64), tensor(57.6341, dtype=torch.float64), tensor(61.2105, dtype=torch.float64), tensor(60.3851, dtype=torch.float64), tensor(59.8349, dtype=torch.float64), tensor(64.0990, dtype=torch.float64), tensor(59.9725, dtype=torch.float64), tensor(62.9986, dtype=torch.float64), tensor(58.0468, dtype=torch.float64), tensor(62.9986, dtype=torch.float64), tensor(58.0468, dtype=torch.float64), tensor(60.1100, dtype=torch.float64), tensor(60.5227, dtype=torch.float64), tensor(61.4856, dtype=torch.float64), tensor(57.7717, dtype=torch.float64), tensor(59.6974, dtype=torch.float64), tensor(57.0839, dtype=torch.float64), tensor(59.2847, dtype=torch.float64), tensor(59.2847, dtype=torch.float64), tensor(60.2476, dtype=torch.float64), tensor(61.8982, dtype=torch.float64), tensor(60.6602, dtype=torch.float64), tensor(65.7497, dtype=torch.float64), tensor(61.0729, dtype=torch.float64), tensor(63.2737, dtype=torch.float64), tensor(58.0468, dtype=torch.float64), tensor(63.5488, dtype=torch.float64), tensor(61.4856, dtype=torch.float64), tensor(63.1362, dtype=torch.float64), tensor(61.4856, dtype=torch.float64), tensor(60.9354, dtype=torch.float64), tensor(60.2476, dtype=torch.float64), tensor(60.5227, dtype=torch.float64), tensor(63.6864, dtype=torch.float64), tensor(64.0990, dtype=torch.float64), tensor(60.6602, dtype=torch.float64), tensor(55.2957, dtype=torch.float64), tensor(63.9615, dtype=torch.float64), tensor(61.3480, dtype=torch.float64), tensor(60.7978, dtype=torch.float64), tensor(65.8872, dtype=torch.float64), tensor(60.6602, dtype=torch.float64), tensor(59.5598, dtype=torch.float64), tensor(61.6231, dtype=torch.float64), tensor(66.4374, dtype=torch.float64), tensor(62.8611, dtype=torch.float64), tensor(61.3480, dtype=torch.float64), tensor(66.2999, dtype=torch.float64), tensor(62.8611, dtype=torch.float64), tensor(62.8611, dtype=torch.float64), tensor(62.1733, dtype=torch.float64), tensor(62.7235, dtype=torch.float64), tensor(62.1733, dtype=torch.float64), tensor(63.5488, dtype=torch.float64), tensor(64.3741, dtype=torch.float64), tensor(64.5117, dtype=torch.float64), tensor(66.9876, dtype=torch.float64), tensor(62.9986, dtype=torch.float64), tensor(66.4374, dtype=torch.float64), tensor(60.6602, dtype=torch.float64), tensor(67.1252, dtype=torch.float64), tensor(62.1733, dtype=torch.float64), tensor(62.4484, dtype=torch.float64), tensor(65.7497, dtype=torch.float64), tensor(69.0509, dtype=torch.float64), tensor(63.6864, dtype=torch.float64), tensor(68.6382, dtype=torch.float64), tensor(64.5117, dtype=torch.float64), tensor(64.0990, dtype=torch.float64), tensor(64.9243, dtype=torch.float64), tensor(63.4113, dtype=torch.float64), tensor(61.4856, dtype=torch.float64), tensor(57.3590, dtype=torch.float64), tensor(62.4484, dtype=torch.float64), tensor(63.4113, dtype=torch.float64), tensor(61.4856, dtype=torch.float64), tensor(63.9615, dtype=torch.float64), tensor(61.3480, dtype=torch.float64), tensor(62.9986, dtype=torch.float64), tensor(65.7497, dtype=torch.float64), tensor(63.9615, dtype=torch.float64), tensor(65.3370, dtype=torch.float64), tensor(65.1994, dtype=torch.float64), tensor(64.7868, dtype=torch.float64), tensor(64.0990, dtype=torch.float64), tensor(61.7607, dtype=torch.float64), tensor(62.3109, dtype=torch.float64), tensor(67.8129, dtype=torch.float64), tensor(64.5117, dtype=torch.float64), tensor(66.2999, dtype=torch.float64), tensor(66.1623, dtype=torch.float64), tensor(62.1733, dtype=torch.float64), tensor(64.9243, dtype=torch.float64), tensor(63.9615, dtype=torch.float64), tensor(64.0990, dtype=torch.float64), tensor(69.7387, dtype=torch.float64), tensor(65.4746, dtype=torch.float64), tensor(68.2256, dtype=torch.float64), tensor(65.7497, dtype=torch.float64), tensor(68.5007, dtype=torch.float64), tensor(61.4856, dtype=torch.float64), tensor(64.7868, dtype=torch.float64), tensor(63.9615, dtype=torch.float64), tensor(66.7125, dtype=torch.float64), tensor(63.6864, dtype=torch.float64), tensor(63.6864, dtype=torch.float64), tensor(66.2999, dtype=torch.float64), tensor(67.1252, dtype=torch.float64), tensor(67.5378, dtype=torch.float64), tensor(63.9615, dtype=torch.float64), tensor(67.1252, dtype=torch.float64), tensor(64.3741, dtype=torch.float64), tensor(64.9243, dtype=torch.float64), tensor(63.8239, dtype=torch.float64), tensor(65.8872, dtype=torch.float64), tensor(64.5117, dtype=torch.float64), tensor(65.3370, dtype=torch.float64), tensor(66.9876, dtype=torch.float64), tensor(64.2366, dtype=torch.float64), tensor(63.4113, dtype=torch.float64), tensor(58.7345, dtype=torch.float64), tensor(66.2999, dtype=torch.float64), tensor(64.2366, dtype=torch.float64), tensor(65.1994, dtype=torch.float64), tensor(62.9986, dtype=torch.float64), tensor(63.4113, dtype=torch.float64), tensor(63.5488, dtype=torch.float64), tensor(65.1994, dtype=torch.float64), tensor(66.1623, dtype=torch.float64)]
Training complete in 1597.0m  9s
Best val Acc: 69.7387%